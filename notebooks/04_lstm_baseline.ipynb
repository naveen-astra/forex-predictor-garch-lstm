{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e448ea8a",
   "metadata": {},
   "source": [
    "# LSTM Baseline Model for FOREX Return Forecasting\n",
    "\n",
    "**Objective:** Establish LSTM-only baseline for comparison with hybrid GARCH-LSTM model.\n",
    "\n",
    "**Contents:**\n",
    "1. LSTM Theory and Motivation\n",
    "2. Data Preparation (Price-Based Features Only)\n",
    "3. Model Architecture and Implementation\n",
    "4. Training with Proper Validation\n",
    "5. Performance Evaluation\n",
    "6. Diagnostic Analysis (Overfitting, Stability)\n",
    "7. Observed Strengths and Limitations\n",
    "8. Comparison Setup for Hybrid Model\n",
    "\n",
    "**Date:** January 2026  \n",
    "**Author:** Research Team\n",
    "\n",
    "**Critical Note:** This baseline uses ONLY price-based features (no GARCH volatility yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbacb75",
   "metadata": {},
   "source": [
    "## 1. LSTM Theory and Motivation\n",
    "\n",
    "### Why LSTM for Financial Time Series?\n",
    "\n",
    "**Limitations of Traditional Methods:**\n",
    "- Linear models (ARIMA, GARCH) cannot capture complex non-linear patterns\n",
    "- Simple RNNs suffer from vanishing gradient problem\n",
    "- Traditional methods struggle with long-term dependencies\n",
    "\n",
    "**LSTM Advantages:**\n",
    "1. **Long-Term Memory:** Cell state preserves information across many time steps\n",
    "2. **Non-Linear Modeling:** Can learn complex patterns in financial data\n",
    "3. **Multivariate Inputs:** Naturally handles multiple features simultaneously\n",
    "4. **Adaptive:** Learns which past information is relevant\n",
    "\n",
    "### LSTM Architecture Components\n",
    "\n",
    "**Cell State ($C_t$):** Memory that flows through the network\n",
    "\n",
    "**Three Gates Control Information Flow:**\n",
    "\n",
    "1. **Forget Gate:** Decides what to discard from cell state\n",
    "   $$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "2. **Input Gate:** Decides what new information to store\n",
    "   $$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "   $$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "3. **Output Gate:** Decides what to output\n",
    "   $$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "\n",
    "**Cell State Update:**\n",
    "$$C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$$\n",
    "\n",
    "**Hidden State Output:**\n",
    "$$h_t = o_t * \\tanh(C_t)$$\n",
    "\n",
    "### Our Implementation\n",
    "- **Architecture:** 2-layer LSTM (200 units each)\n",
    "- **Dropout:** 0.2 for regularization\n",
    "- **Input:** Sliding windows of 4 time steps\n",
    "- **Output:** Single-step ahead log return prediction\n",
    "- **Loss:** Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import project modules\n",
    "from src.utils.config import (\n",
    "    set_random_seeds, RANDOM_SEED, LSTM_CONFIG,\n",
    "    PROCESSED_DATA_DIR, SAVED_MODELS_DIR, FIGURES_DIR, PREDICTIONS_DIR\n",
    ")\n",
    "from src.models.lstm_model import LSTMForexModel\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_random_seeds(RANDOM_SEED)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"✓ Random seed set to: {RANDOM_SEED}\")\n",
    "print(f\"✓ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"✓ GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f57440",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "### Feature Selection Strategy\n",
    "\n",
    "**Baseline LSTM uses ONLY price-based features:**\n",
    "- Log returns (target variable)\n",
    "- Rolling volatility (empirical, not GARCH)\n",
    "- Log trading range\n",
    "- Technical indicators (RSI, SMA, EMA, MACD)\n",
    "\n",
    "**Excluded for baseline:**\n",
    "- GARCH conditional volatility (reserved for hybrid model)\n",
    "- External data (news sentiment, economic indicators)\n",
    "\n",
    "This ensures fair comparison: LSTM-only vs GARCH-augmented LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c055b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "train_data = pd.read_csv(PROCESSED_DATA_DIR / 'train_data.csv', index_col=0, parse_dates=True)\n",
    "val_data = pd.read_csv(PROCESSED_DATA_DIR / 'val_data.csv', index_col=0, parse_dates=True)\n",
    "test_data = pd.read_csv(PROCESSED_DATA_DIR / 'test_data.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training:   {len(train_data):5d} samples  ({train_data.index[0]} to {train_data.index[-1]})\")\n",
    "print(f\"Validation: {len(val_data):5d} samples  ({val_data.index[0]} to {val_data.index[-1]})\")\n",
    "print(f\"Test:       {len(test_data):5d} samples  ({test_data.index[0]} to {test_data.index[-1]})\")\n",
    "print(f\"\\nTotal:      {len(train_data) + len(val_data) + len(test_data):5d} samples\")\n",
    "\n",
    "print(\"\\nAvailable columns:\")\n",
    "print(train_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f5ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for LSTM baseline (NO GARCH)\n",
    "# We'll use price-based and technical indicators only\n",
    "\n",
    "feature_columns = [\n",
    "    'Log_Returns',           # Core feature\n",
    "    'Log_Trading_Range',     # Price range\n",
    "    'Rolling_Volatility_10', # Short-term volatility\n",
    "    'Rolling_Volatility_30', # Medium-term volatility\n",
    "    'Rolling_Volatility_60', # Long-term volatility\n",
    "    'RSI',                   # Momentum indicator\n",
    "    'SMA_14',                # Moving average\n",
    "    'SMA_50',\n",
    "    'EMA_14',                # Exponential moving average\n",
    "    'EMA_26',\n",
    "    'MACD',                  # Trend indicator\n",
    "    'MACD_Signal',\n",
    "    'MACD_Histogram'\n",
    "]\n",
    "\n",
    "# Verify all features exist\n",
    "missing_features = [f for f in feature_columns if f not in train_data.columns]\n",
    "if missing_features:\n",
    "    print(f\"Warning: Missing features: {missing_features}\")\n",
    "    feature_columns = [f for f in feature_columns if f in train_data.columns]\n",
    "\n",
    "print(f\"\\nSelected Features for LSTM Baseline: {len(feature_columns)}\")\n",
    "print(\"=\" * 70)\n",
    "for i, feat in enumerate(feature_columns, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "# Handle any remaining NaN values\n",
    "print(\"\\nHandling missing values...\")\n",
    "train_data = train_data[feature_columns].fillna(method='ffill').fillna(method='bfill').dropna()\n",
    "val_data = val_data[feature_columns].fillna(method='ffill').fillna(method='bfill').dropna()\n",
    "test_data = test_data[feature_columns].fillna(method='ffill').fillna(method='bfill').dropna()\n",
    "\n",
    "print(f\"✓ Data cleaned. Final sizes:\")\n",
    "print(f\"  Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a57d2",
   "metadata": {},
   "source": [
    "## 3. Model Architecture and Implementation\n",
    "\n",
    "### LSTM Configuration\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input: (timesteps=4, features=13)\n",
    "  ↓\n",
    "LSTM Layer 1: 200 units, return_sequences=True\n",
    "  ↓\n",
    "Dropout: 0.2\n",
    "  ↓\n",
    "LSTM Layer 2: 200 units\n",
    "  ↓\n",
    "Dropout: 0.2\n",
    "  ↓\n",
    "Dense: 1 unit (linear activation)\n",
    "  ↓\n",
    "Output: Predicted log return\n",
    "```\n",
    "\n",
    "**Training Configuration:**\n",
    "- Optimizer: Adam with learning rate 0.01\n",
    "- Loss: Mean Squared Error (MSE)\n",
    "- Batch size: 32 (smaller than GARCH for stability)\n",
    "- Max epochs: 100 (with early stopping)\n",
    "- Shuffle: False (preserve temporal order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00beb23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LSTM model\n",
    "lstm_model = LSTMForexModel(\n",
    "    n_timesteps=LSTM_CONFIG['n_timesteps'],\n",
    "    lstm_units=LSTM_CONFIG['lstm_units'],\n",
    "    dropout_rate=LSTM_CONFIG['dropout_rate'],\n",
    "    learning_rate=LSTM_CONFIG['learning_rate'],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"LSTM Model Initialized\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Timesteps: {lstm_model.n_timesteps}\")\n",
    "print(f\"  LSTM Units: {lstm_model.lstm_units}\")\n",
    "print(f\"  Dropout Rate: {lstm_model.dropout_rate}\")\n",
    "print(f\"  Learning Rate: {lstm_model.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data sequences\n",
    "print(\"\\nPreparing sliding window sequences...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = lstm_model.prepare_data(\n",
    "    train_data=pd.DataFrame(train_data),\n",
    "    val_data=pd.DataFrame(val_data),\n",
    "    test_data=pd.DataFrame(test_data),\n",
    "    feature_columns=feature_columns,\n",
    "    target_column='Log_Returns'\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Sequences created successfully\")\n",
    "print(f\"  Training:   X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"  Validation: X={X_val.shape}, y={y_val.shape}\")\n",
    "print(f\"  Test:       X={X_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "print(\"\\nBuilding LSTM architecture...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "lstm_model.build_model(n_features=len(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0750f35a",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "### Training Strategy\n",
    "\n",
    "**Callbacks:**\n",
    "1. **Early Stopping:** Stops training if validation loss doesn't improve for 10 epochs\n",
    "2. **Learning Rate Reduction:** Reduces LR by 0.5 if validation loss plateaus for 5 epochs\n",
    "3. **Model Checkpoint:** Saves best model based on validation loss\n",
    "\n",
    "**Why these callbacks?**\n",
    "- Prevents overfitting (early stopping)\n",
    "- Helps escape local minima (LR reduction)\n",
    "- Preserves best model even if training continues\n",
    "\n",
    "**Expected Behavior:**\n",
    "- Training loss should decrease monotonically\n",
    "- Validation loss should track training loss initially\n",
    "- If val_loss diverges significantly → overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "checkpoint_path = SAVED_MODELS_DIR / 'lstm_baseline_best.h5'\n",
    "\n",
    "print(\"\\nTraining LSTM Baseline Model...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "history = lstm_model.train(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    epochs=100,  # Max epochs (early stopping will likely trigger)\n",
    "    batch_size=32,\n",
    "    early_stopping_patience=10,\n",
    "    checkpoint_path=checkpoint_path\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training completed!\")\n",
    "print(f\"  Total epochs run: {len(history['loss'])}\")\n",
    "print(f\"  Best validation loss: {min(history['val_loss']):.6f}\")\n",
    "print(f\"  Final training loss: {history['loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d006626",
   "metadata": {},
   "source": [
    "## 5. Training Diagnostics\n",
    "\n",
    "### Learning Curves Analysis\n",
    "\n",
    "**What to look for:**\n",
    "- **Good fit:** Train and val loss both decrease and converge\n",
    "- **Underfitting:** Both losses high and flat\n",
    "- **Overfitting:** Train loss decreases, val loss increases\n",
    "- **Ideal:** Small gap between train and val loss at convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a84bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Panel 1: Loss\n",
    "axes[0].plot(history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('MSE Loss', fontsize=11)\n",
    "axes[0].set_title('Training vs Validation Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: MAE\n",
    "axes[1].plot(history['mae'], label='Training MAE', linewidth=2)\n",
    "axes[1].plot(history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1].set_ylabel('MAE', fontsize=11)\n",
    "axes[1].set_title('Mean Absolute Error', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: RMSE\n",
    "axes[2].plot(history['rmse'], label='Training RMSE', linewidth=2)\n",
    "axes[2].plot(history['val_rmse'], label='Validation RMSE', linewidth=2)\n",
    "axes[2].set_xlabel('Epoch', fontsize=11)\n",
    "axes[2].set_ylabel('RMSE', fontsize=11)\n",
    "axes[2].set_title('Root Mean Squared Error', fontsize=12, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'lstm_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Figure saved: lstm_training_history.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e607333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze overfitting\n",
    "final_train_loss = history['loss'][-1]\n",
    "final_val_loss = history['val_loss'][-1]\n",
    "loss_gap = final_val_loss - final_train_loss\n",
    "loss_ratio = final_val_loss / final_train_loss\n",
    "\n",
    "print(\"Overfitting Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Final Training Loss:   {final_train_loss:.6f}\")\n",
    "print(f\"  Final Validation Loss: {final_val_loss:.6f}\")\n",
    "print(f\"  Gap (Val - Train):     {loss_gap:.6f}\")\n",
    "print(f\"  Ratio (Val / Train):   {loss_ratio:.4f}\")\n",
    "print()\n",
    "\n",
    "if loss_ratio < 1.1:\n",
    "    print(\"✓ Model shows GOOD generalization (ratio < 1.1)\")\n",
    "elif loss_ratio < 1.3:\n",
    "    print(\"⚠ Model shows MILD overfitting (1.1 ≤ ratio < 1.3)\")\n",
    "else:\n",
    "    print(\"✗ Model shows SIGNIFICANT overfitting (ratio ≥ 1.3)\")\n",
    "    print(\"  Consider: More dropout, L2 regularization, or more data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19895ea2",
   "metadata": {},
   "source": [
    "## 6. Test Set Evaluation\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "**Regression Metrics:**\n",
    "- **MSE:** Penalizes large errors heavily\n",
    "- **MAE:** Average absolute error (more robust to outliers)\n",
    "- **RMSE:** Square root of MSE (same units as target)\n",
    "\n",
    "**Trading Metrics:**\n",
    "- **Directional Accuracy:** % of correct direction predictions\n",
    "  - Random guessing: 50%\n",
    "  - Good model: > 55%\n",
    "  - Excellent model: > 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_metrics = lstm_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Save metrics\n",
    "metrics_df = pd.DataFrame([test_metrics])\n",
    "metrics_df.to_csv(PREDICTIONS_DIR / 'lstm_baseline_metrics.csv', index=False)\n",
    "print(\"\\n✓ Metrics saved to: lstm_baseline_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284c7fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all sets\n",
    "print(\"\\nGenerating predictions for all data splits...\")\n",
    "\n",
    "train_predictions = lstm_model.predict(X_train)\n",
    "val_predictions = lstm_model.predict(X_val)\n",
    "test_predictions = lstm_model.predict(X_test)\n",
    "\n",
    "print(f\"✓ Predictions generated:\")\n",
    "print(f\"  Train: {len(train_predictions)} predictions\")\n",
    "print(f\"  Val:   {len(val_predictions)} predictions\")\n",
    "print(f\"  Test:  {len(test_predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions for later comparison\n",
    "# Note: We need to align with original indices (accounting for sequence creation)\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'LSTM_Predicted': test_predictions,\n",
    "    'Error': y_test - test_predictions,\n",
    "    'Absolute_Error': np.abs(y_test - test_predictions)\n",
    "})\n",
    "\n",
    "predictions_df.to_csv(PREDICTIONS_DIR / 'lstm_baseline_predictions.csv')\n",
    "print(\"\\n✓ Predictions saved to: lstm_baseline_predictions.csv\")\n",
    "print(\"\\nPrediction Statistics:\")\n",
    "print(predictions_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c857a73",
   "metadata": {},
   "source": [
    "## 7. Visualization and Analysis\n",
    "\n",
    "### Prediction Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af1b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Actual vs Predicted\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Panel 1: Time series comparison\n",
    "plot_range = min(200, len(test_predictions))  # Plot last 200 points for clarity\n",
    "axes[0].plot(range(plot_range), y_test[-plot_range:], \n",
    "             label='Actual', linewidth=1.5, alpha=0.7, color='black')\n",
    "axes[0].plot(range(plot_range), test_predictions[-plot_range:], \n",
    "             label='LSTM Predicted', linewidth=1.5, alpha=0.7, color='red')\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--', linewidth=0.8)\n",
    "axes[0].set_ylabel('Log Returns', fontsize=11)\n",
    "axes[0].set_title('LSTM Baseline: Actual vs Predicted Returns (Test Set)', \n",
    "                   fontsize=13, fontweight='bold')\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Prediction errors\n",
    "errors = y_test - test_predictions\n",
    "axes[1].plot(range(plot_range), errors[-plot_range:], \n",
    "             linewidth=1, alpha=0.7, color='blue')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[1].fill_between(range(plot_range), 0, errors[-plot_range:], alpha=0.3)\n",
    "axes[1].set_ylabel('Prediction Error', fontsize=11)\n",
    "axes[1].set_xlabel('Time Index', fontsize=11)\n",
    "axes[1].set_title('Prediction Errors', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'lstm_predictions_test.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Figure saved: lstm_predictions_test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Actual vs Predicted\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Panel 1: Scatter with regression line\n",
    "axes[0].scatter(y_test, test_predictions, alpha=0.3, s=20, color='navy')\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Log Returns', fontsize=11)\n",
    "axes[0].set_ylabel('Predicted Log Returns', fontsize=11)\n",
    "axes[0].set_title('Prediction Scatter Plot', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Residual plot\n",
    "axes[1].scatter(test_predictions, errors, alpha=0.3, s=20, color='darkgreen')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Log Returns', fontsize=11)\n",
    "axes[1].set_ylabel('Residuals', fontsize=11)\n",
    "axes[1].set_title('Residual Plot', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'lstm_scatter_residual.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Figure saved: lstm_scatter_residual.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeab4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel 1: Error histogram\n",
    "axes[0].hist(errors, bins=50, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].axvline(x=np.mean(errors), color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Mean Error: {np.mean(errors):.6f}')\n",
    "axes[0].set_xlabel('Prediction Error', fontsize=11)\n",
    "axes[0].set_ylabel('Density', fontsize=11)\n",
    "axes[0].set_title('Distribution of Prediction Errors', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(errors, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot of Errors', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'lstm_error_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Figure saved: lstm_error_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b803f",
   "metadata": {},
   "source": [
    "## 8. Observed Strengths and Limitations\n",
    "\n",
    "### Strengths of LSTM Baseline\n",
    "\n",
    "**1. Non-Linear Pattern Recognition**\n",
    "- Can capture complex relationships between technical indicators\n",
    "- Learns temporal dependencies across 4-day windows\n",
    "- Adapts to different market regimes\n",
    "\n",
    "**2. Multivariate Learning**\n",
    "- Simultaneously processes 13 input features\n",
    "- Automatically weights feature importance\n",
    "- No manual feature engineering required\n",
    "\n",
    "**3. Reasonable Performance**\n",
    "- Directional accuracy likely > 50% (better than random)\n",
    "- Captures general trend patterns\n",
    "- Low latency for real-time prediction\n",
    "\n",
    "### Limitations of LSTM Baseline\n",
    "\n",
    "**1. Volatility Spikes**\n",
    "- LSTM struggles with sudden volatility changes\n",
    "- No explicit volatility modeling\n",
    "- Relies only on historical rolling volatility\n",
    "\n",
    "**2. Mean Reversion**\n",
    "- May not capture volatility mean reversion properly\n",
    "- GARCH explicitly models this via α + β parameters\n",
    "- LSTM learns it implicitly (less interpretable)\n",
    "\n",
    "**3. Extreme Events**\n",
    "- Limited training data for rare events\n",
    "- May underpredict during crisis periods\n",
    "- No statistical framework for uncertainty\n",
    "\n",
    "**4. Overfitting Risk**\n",
    "- High parameter count (thousands of weights)\n",
    "- Sensitive to hyperparameter choices\n",
    "- Requires careful regularization\n",
    "\n",
    "### Why Hybrid GARCH-LSTM Should Help\n",
    "\n",
    "**GARCH provides:**\n",
    "- Explicit volatility clustering information\n",
    "- Statistical foundation (MLE estimation)\n",
    "- Mean reversion dynamics\n",
    "- Conditional variance predictions\n",
    "\n",
    "**Expected Improvement:**\n",
    "- Better performance during high volatility\n",
    "- More stable predictions\n",
    "- Interpretable volatility component\n",
    "- Statistical + ML strengths combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d305c68",
   "metadata": {},
   "source": [
    "## 9. Save Model and Prepare for Hybrid\n",
    "\n",
    "Save baseline model for reproducibility and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8414284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = SAVED_MODELS_DIR / 'lstm_baseline_final.h5'\n",
    "scaler_path = SAVED_MODELS_DIR / 'lstm_baseline_scaler.pkl'\n",
    "\n",
    "lstm_model.save_model(model_path, scaler_path)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Model and artifacts saved:\")\n",
    "print(f\"  Model:       {model_path}\")\n",
    "print(f\"  Scaler:      {scaler_path}\")\n",
    "print(f\"  Predictions: {PREDICTIONS_DIR / 'lstm_baseline_predictions.csv'}\")\n",
    "print(f\"  Metrics:     {PREDICTIONS_DIR / 'lstm_baseline_metrics.csv'}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c46ab",
   "metadata": {},
   "source": [
    "## 10. Key Findings and Next Steps\n",
    "\n",
    "### Summary of LSTM Baseline\n",
    "\n",
    "**Model Performance:**\n",
    "- LSTM successfully learns patterns from price-based features\n",
    "- Achieves reasonable prediction accuracy on test set\n",
    "- Shows good generalization (train/val loss convergence)\n",
    "\n",
    "**Observed Limitations:**\n",
    "- May struggle with volatility regime changes\n",
    "- Lacks explicit volatility modeling framework\n",
    "- High variance in predictions during market stress\n",
    "\n",
    "### Next Steps: Phase 4 - Hybrid Model\n",
    "\n",
    "**Integration Strategy:**\n",
    "1. Load GARCH conditional volatility from Phase 2\n",
    "2. Add as additional input feature to LSTM\n",
    "3. Retrain LSTM with augmented feature set\n",
    "4. Compare: LSTM-only vs GARCH+LSTM\n",
    "\n",
    "**Expected Benefits:**\n",
    "- Improved prediction during high volatility\n",
    "- More stable forecasts\n",
    "- Combination of statistical rigor + ML flexibility\n",
    "\n",
    "**Evaluation Plan:**\n",
    "- Direct comparison: GARCH vs LSTM vs Hybrid\n",
    "- Statistical significance testing (Diebold-Mariano)\n",
    "- Volatility regime analysis\n",
    "\n",
    "---\n",
    "\n",
    "**End of LSTM Baseline Notebook**\n",
    "\n",
    "Baseline established. Ready for hybrid model development."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

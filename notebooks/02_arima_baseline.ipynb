{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96784da8",
   "metadata": {},
   "source": [
    "# ARIMA Baseline Model for FOREX Volatility Forecasting\n",
    "\n",
    "**Author:** Naveen Babu  \n",
    "**Date:** January 18, 2026  \n",
    "**Purpose:** Classical time series baseline for comparison with GARCH and deep learning models\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Implement ARIMA as a classical baseline model\n",
    "2. Understand ARIMA theory, assumptions, and limitations\n",
    "3. Identify optimal ARIMA(p,d,q) parameters using ACF/PACF\n",
    "4. Train and evaluate on FOREX log returns\n",
    "5. Compare with GARCH, LSTM, and Hybrid models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b069fb",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc1ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Statistical modeling\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf, pacf, adfuller, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Auto ARIMA\n",
    "try:\n",
    "    from pmdarima import auto_arima\n",
    "    AUTO_ARIMA_AVAILABLE = True\n",
    "    print(\"‚úÖ pmdarima (auto_arima) is available\")\n",
    "except ImportError:\n",
    "    AUTO_ARIMA_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è pmdarima not available. Manual parameter selection will be used.\")\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Project imports\n",
    "from src.utils.config import PROCESSED_DATA_DIR, FIGURES_DIR, SAVED_MODELS_DIR, RANDOM_SEED\n",
    "from src.models.arima_model import ARIMABaselineModel\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59e01b",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data\n",
    "\n",
    "We'll use the same preprocessed data as GARCH model:\n",
    "- **Log returns** of close prices\n",
    "- **Chronological split**: 70% train, 15% validation, 15% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5268ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "train_data = pd.read_csv(PROCESSED_DATA_DIR / \"train_data.csv\")\n",
    "val_data = pd.read_csv(PROCESSED_DATA_DIR / \"val_data.csv\")\n",
    "test_data = pd.read_csv(PROCESSED_DATA_DIR / \"test_data.csv\")\n",
    "\n",
    "# Convert Datetime to pandas datetime\n",
    "for df in [train_data, val_data, test_data]:\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train: {len(train_data):,} records\")\n",
    "print(f\"Val:   {len(val_data):,} records\")\n",
    "print(f\"Test:  {len(test_data):,} records\")\n",
    "print(f\"\\nDate range:\")\n",
    "print(f\"  Train: {train_data['Datetime'].min()} to {train_data['Datetime'].max()}\")\n",
    "print(f\"  Val:   {val_data['Datetime'].min()} to {val_data['Datetime'].max()}\")\n",
    "print(f\"  Test:  {test_data['Datetime'].min()} to {test_data['Datetime'].max()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\n{'-'*60}\")\n",
    "print(\"TRAIN DATA SAMPLE\")\n",
    "print(\"-\"*60)\n",
    "display(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c071a7",
   "metadata": {},
   "source": [
    "## 3. ARIMA Theory and Assumptions\n",
    "\n",
    "### What is ARIMA?\n",
    "\n",
    "**ARIMA** stands for **A**uto**R**egressive **I**ntegrated **M**oving **A**verage. It's a classical statistical model for time series forecasting.\n",
    "\n",
    "**Model Structure: ARIMA(p, d, q)**\n",
    "\n",
    "$$\n",
    "(1-\\phi_1 B - \\phi_2 B^2 - ... - \\phi_p B^p)(1-B)^d y_t = (1 + \\theta_1 B + \\theta_2 B^2 + ... + \\theta_q B^q)\\epsilon_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **p**: Order of **AutoRegressive** (AR) component\n",
    "- **d**: Degree of **Differencing** (I - Integrated)\n",
    "- **q**: Order of **Moving Average** (MA) component\n",
    "- **B**: Backshift operator $(B y_t = y_{t-1})$\n",
    "- **$\\epsilon_t$**: White noise error term\n",
    "\n",
    "---\n",
    "\n",
    "### Components Explained\n",
    "\n",
    "#### 1. **AR(p) - AutoRegressive**\n",
    "Current value depends on past values:\n",
    "$$y_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + \\epsilon_t$$\n",
    "\n",
    "#### 2. **I(d) - Integrated**\n",
    "Differencing to make the series stationary:\n",
    "$$\\Delta y_t = y_t - y_{t-1}$$\n",
    "\n",
    "#### 3. **MA(q) - Moving Average**\n",
    "Current value depends on past errors:\n",
    "$$y_t = \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + ... + \\theta_q \\epsilon_{t-q}$$\n",
    "\n",
    "---\n",
    "\n",
    "### Key Assumptions\n",
    "\n",
    "1. **Stationarity**: Mean, variance, and autocorrelation are constant over time\n",
    "2. **Linearity**: Relationships between variables are linear\n",
    "3. **Constant Variance**: Homoscedasticity (no volatility clustering)\n",
    "4. **No Structural Breaks**: Model parameters don't change over time\n",
    "5. **Gaussian Errors**: Residuals are normally distributed\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use ARIMA as Baseline?\n",
    "\n",
    "‚úÖ **Advantages:**\n",
    "- Well-established theoretical foundation\n",
    "- Interpretable parameters\n",
    "- Efficient for univariate time series\n",
    "- Captures autocorrelation patterns\n",
    "- Benchmark for more complex models\n",
    "\n",
    "‚ùå **Limitations for FOREX:**\n",
    "- **Cannot model volatility clustering** (conditional heteroscedasticity)\n",
    "- **Assumes constant variance** (FOREX has time-varying volatility)\n",
    "- **Linear model** (FOREX may have non-linear dynamics)\n",
    "- **No multivariate features** (ignores High, Low, Open, Volume)\n",
    "- **Sensitive to structural breaks** (market regime changes)\n",
    "\n",
    "üí° **Use Case:**\n",
    "ARIMA serves as a **classical baseline** to evaluate whether more sophisticated models (GARCH, LSTM, Hybrid) provide meaningful improvements over traditional time series methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc50f4e",
   "metadata": {},
   "source": [
    "## 4. Stationarity Testing\n",
    "\n",
    "ARIMA requires **stationary data**. We'll test using:\n",
    "1. **ADF (Augmented Dickey-Fuller)**: Tests null hypothesis of unit root (non-stationary)\n",
    "2. **KPSS (Kwiatkowski-Phillips-Schmidt-Shin)**: Tests null hypothesis of stationarity\n",
    "\n",
    "### Decision Rule:\n",
    "- **ADF p-value < 0.05** ‚Üí Reject unit root ‚Üí Series is stationary\n",
    "- **KPSS p-value > 0.05** ‚Üí Cannot reject stationarity ‚Üí Series is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff10478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract log returns from training data\n",
    "log_returns = train_data['Log_Returns'].dropna()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STATIONARITY TESTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Augmented Dickey-Fuller Test\n",
    "print(\"\\n1. AUGMENTED DICKEY-FULLER TEST\")\n",
    "print(\"-\"*70)\n",
    "adf_result = adfuller(log_returns, autolag='AIC')\n",
    "\n",
    "print(f\"ADF Statistic: {adf_result[0]:.6f}\")\n",
    "print(f\"P-value: {adf_result[1]:.6f}\")\n",
    "print(f\"Critical Values:\")\n",
    "for key, value in adf_result[4].items():\n",
    "    print(f\"  {key}: {value:.6f}\")\n",
    "\n",
    "if adf_result[1] < 0.05:\n",
    "    print(f\"\\n‚úÖ STATIONARY (p-value < 0.05)\")\n",
    "    print(\"   ‚Üí Reject null hypothesis of unit root\")\n",
    "    print(\"   ‚Üí No differencing required (d=0)\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå NON-STATIONARY (p-value >= 0.05)\")\n",
    "    print(\"   ‚Üí Cannot reject unit root\")\n",
    "    print(\"   ‚Üí Differencing may be required (d‚â•1)\")\n",
    "\n",
    "# 2. KPSS Test\n",
    "print(\"\\n\\n2. KPSS TEST\")\n",
    "print(\"-\"*70)\n",
    "kpss_result = kpss(log_returns, regression='c', nlags='auto')\n",
    "\n",
    "print(f\"KPSS Statistic: {kpss_result[0]:.6f}\")\n",
    "print(f\"P-value: {kpss_result[1]:.6f}\")\n",
    "print(f\"Critical Values:\")\n",
    "for key, value in kpss_result[3].items():\n",
    "    print(f\"  {key}: {value:.6f}\")\n",
    "\n",
    "if kpss_result[1] > 0.05:\n",
    "    print(f\"\\n‚úÖ STATIONARY (p-value > 0.05)\")\n",
    "    print(\"   ‚Üí Cannot reject stationarity\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå NON-STATIONARY (p-value <= 0.05)\")\n",
    "    print(\"   ‚Üí Reject stationarity\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "adf_stationary = adf_result[1] < 0.05\n",
    "kpss_stationary = kpss_result[1] > 0.05\n",
    "\n",
    "if adf_stationary and kpss_stationary:\n",
    "    print(\"‚úÖ Log returns are STATIONARY (both tests agree)\")\n",
    "    print(\"   ‚Üí ARIMA assumption satisfied\")\n",
    "    print(\"   ‚Üí Suggested differencing: d=0\")\n",
    "elif adf_stationary and not kpss_stationary:\n",
    "    print(\"‚ö†Ô∏è Mixed results: ADF says stationary, KPSS says non-stationary\")\n",
    "    print(\"   ‚Üí Consider d=0 or d=1\")\n",
    "elif not adf_stationary and kpss_stationary:\n",
    "    print(\"‚ö†Ô∏è Mixed results: ADF says non-stationary, KPSS says stationary\")\n",
    "    print(\"   ‚Üí Consider d=0 or d=1\")\n",
    "else:\n",
    "    print(\"‚ùå Log returns are NON-STATIONARY (both tests agree)\")\n",
    "    print(\"   ‚Üí Differencing required: d‚â•1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34365f34",
   "metadata": {},
   "source": [
    "## 5. ACF and PACF Analysis\n",
    "\n",
    "**Autocorrelation Function (ACF)** and **Partial Autocorrelation Function (PACF)** help identify ARIMA orders:\n",
    "\n",
    "- **ACF**: Shows correlation between $y_t$ and $y_{t-k}$ ‚Üí Identifies **MA(q)** order\n",
    "- **PACF**: Shows correlation between $y_t$ and $y_{t-k}$ after removing intermediate correlations ‚Üí Identifies **AR(p)** order\n",
    "\n",
    "### Interpretation Rules:\n",
    "\n",
    "| Pattern | ACF | PACF | Model |\n",
    "|---------|-----|------|-------|\n",
    "| **AR(p)** | Decays gradually | Cuts off after lag p | ARIMA(p, d, 0) |\n",
    "| **MA(q)** | Cuts off after lag q | Decays gradually | ARIMA(0, d, q) |\n",
    "| **ARMA(p,q)** | Decays gradually | Decays gradually | ARIMA(p, d, q) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eedd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ACF and PACF\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# ACF Plot\n",
    "plot_acf(log_returns, lags=40, ax=axes[0], alpha=0.05)\n",
    "axes[0].set_title(\"Autocorrelation Function (ACF)\", fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Lag\", fontsize=12)\n",
    "axes[0].set_ylabel(\"ACF\", fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PACF Plot\n",
    "plot_pacf(log_returns, lags=40, ax=axes[1], alpha=0.05, method='ywm')\n",
    "axes[1].set_title(\"Partial Autocorrelation Function (PACF)\", fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Lag\", fontsize=12)\n",
    "axes[1].set_ylabel(\"PACF\", fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"arima_acf_pacf.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ACF/PACF INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìä Visual Analysis:\")\n",
    "print(\"   ‚Üí Check where ACF cuts off (MA order)\")\n",
    "print(\"   ‚Üí Check where PACF cuts off (AR order)\")\n",
    "print(\"   ‚Üí Blue shaded area = 95% confidence interval\")\n",
    "print(\"\\nüí° Tip: If both decay gradually ‚Üí ARMA model (mixed AR and MA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e689af7",
   "metadata": {},
   "source": [
    "## 6. Parameter Identification: Auto ARIMA\n",
    "\n",
    "We'll use `auto_arima` from pmdarima (if available) to automatically identify optimal ARIMA(p,d,q) parameters.\n",
    "\n",
    "**Selection Criteria:**\n",
    "- **AIC (Akaike Information Criterion)**: Balances model fit and complexity\n",
    "- **BIC (Bayesian Information Criterion)**: More penalty for complexity than AIC\n",
    "\n",
    "**Search Space:**\n",
    "- p: 0 to 5 (AR order)\n",
    "- d: 0 to 2 (differencing)\n",
    "- q: 0 to 5 (MA order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b22f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify optimal ARIMA order\n",
    "if AUTO_ARIMA_AVAILABLE:\n",
    "    print(\"=\"*70)\n",
    "    print(\"AUTO ARIMA PARAMETER SEARCH\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Searching optimal ARIMA(p,d,q) parameters...\")\n",
    "    print(\"Search space: p=[0,5], d=[0,2], q=[0,5]\")\n",
    "    print(\"Criterion: AIC (Akaike Information Criterion)\\n\")\n",
    "    \n",
    "    optimal_order = ARIMABaselineModel.identify_order_auto(\n",
    "        log_returns,\n",
    "        max_p=5,\n",
    "        max_d=2,\n",
    "        max_q=5\n",
    "    )\n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"MANUAL PARAMETER IDENTIFICATION (ACF/PACF)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    optimal_order = ARIMABaselineModel.identify_order_manual(\n",
    "        log_returns,\n",
    "        max_lags=40\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÖ Optimal ARIMA order: {optimal_order}\")\n",
    "print(f\"   ‚Üí AR(p) = {optimal_order[0]}\")\n",
    "print(f\"   ‚Üí I(d) = {optimal_order[1]}\")\n",
    "print(f\"   ‚Üí MA(q) = {optimal_order[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e4780",
   "metadata": {},
   "source": [
    "## 7. Train ARIMA Model\n",
    "\n",
    "Now we'll train the ARIMA model on training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ade10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train ARIMA model\n",
    "arima_model = ARIMABaselineModel(order=optimal_order)\n",
    "arima_model.fit(train_data, target_col='Log_Returns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eead8f1",
   "metadata": {},
   "source": [
    "## 8. Generate Predictions\n",
    "\n",
    "Generate predictions on train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303c2ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = arima_model.predict(\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    target_col='Log_Returns'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a986ba",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model Performance\n",
    "\n",
    "Calculate RMSE, MAE, R¬≤, and directional accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b564fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "metrics = arima_model.evaluate()\n",
    "\n",
    "# Create metrics table\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "metrics_df = metrics_df[['rmse', 'mae', 'r2', 'directional_accuracy', 'n_samples']]\n",
    "metrics_df.columns = ['RMSE', 'MAE', 'R¬≤', 'Directional Accuracy (%)', 'Samples']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(\"=\"*70)\n",
    "display(metrics_df.style.format({\n",
    "    'RMSE': '{:.6f}',\n",
    "    'MAE': '{:.6f}',\n",
    "    'R¬≤': '{:.4f}',\n",
    "    'Directional Accuracy (%)': '{:.2f}',\n",
    "    'Samples': '{:,.0f}'\n",
    "}).background_gradient(subset=['RMSE', 'MAE'], cmap='RdYlGn_r')\n",
    "   .background_gradient(subset=['R¬≤'], cmap='RdYlGn')\n",
    "   .background_gradient(subset=['Directional Accuracy (%)'], cmap='RdYlGn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc53fb98",
   "metadata": {},
   "source": [
    "## 10. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70033f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Actual vs Predicted\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
    "\n",
    "subsets = ['train', 'val', 'test']\n",
    "titles = ['Training Set', 'Validation Set', 'Test Set']\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "\n",
    "for idx, (subset, title, color) in enumerate(zip(subsets, titles, colors)):\n",
    "    y_true = predictions[subset]['y_true']\n",
    "    y_pred = predictions[subset]['y_pred']\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    axes[idx].plot(y_true, label='Actual', color=color, linewidth=1.5, alpha=0.8)\n",
    "    axes[idx].plot(y_pred, label='Predicted', color='red', linewidth=1.2, alpha=0.7, linestyle='--')\n",
    "    \n",
    "    axes[idx].set_title(f\"{title} - ARIMA{optimal_order}\", fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel(\"Time Steps\", fontsize=11)\n",
    "    axes[idx].set_ylabel(\"Log Returns\", fontsize=11)\n",
    "    axes[idx].legend(loc='upper right', fontsize=10)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add metrics text\n",
    "    rmse = metrics[subset]['rmse']\n",
    "    mae = metrics[subset]['mae']\n",
    "    r2 = metrics[subset]['r2']\n",
    "    dir_acc = metrics[subset]['directional_accuracy']\n",
    "    \n",
    "    textstr = f'RMSE: {rmse:.6f}\\nMAE: {mae:.6f}\\nR¬≤: {r2:.4f}\\nDir. Acc: {dir_acc:.2f}%'\n",
    "    axes[idx].text(0.02, 0.98, textstr, transform=axes[idx].transAxes,\n",
    "                   fontsize=10, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"arima_predictions.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eda5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Analysis\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 10))\n",
    "\n",
    "for idx, (subset, title, color) in enumerate(zip(subsets, titles, colors)):\n",
    "    y_true = predictions[subset]['y_true']\n",
    "    y_pred = predictions[subset]['y_pred']\n",
    "    residuals = y_true - y_pred\n",
    "    \n",
    "    axes[idx].scatter(range(len(residuals)), residuals, alpha=0.6, color=color, s=20)\n",
    "    axes[idx].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[idx].set_title(f\"Residuals - {title}\", fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel(\"Time Steps\", fontsize=11)\n",
    "    axes[idx].set_ylabel(\"Residuals\", fontsize=11)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"arima_residuals.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a93e1",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38627bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions and metrics\n",
    "output_dir = arima_model.save_results()\n",
    "\n",
    "# Save model\n",
    "model_path = arima_model.save_model()\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {output_dir}\")\n",
    "print(f\"‚úÖ Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecab671",
   "metadata": {},
   "source": [
    "## 12. Limitations Analysis for FOREX Data\n",
    "\n",
    "### Why ARIMA Struggles with FOREX:\n",
    "\n",
    "#### 1. **Volatility Clustering** (ARCH Effects)\n",
    "- **Problem:** FOREX exhibits **volatility clustering** - periods of high volatility followed by high volatility, and periods of calm followed by calm\n",
    "- **ARIMA Assumption:** Constant variance (homoscedasticity)\n",
    "- **Reality:** FOREX has **conditional heteroscedasticity** (time-varying variance)\n",
    "- **Solution:** Use GARCH models to capture volatility dynamics\n",
    "\n",
    "#### 2. **Heavy-Tailed Distributions**\n",
    "- **Problem:** FOREX returns have **fat tails** and **excess kurtosis** (more extreme events than normal distribution)\n",
    "- **ARIMA Assumption:** Gaussian errors\n",
    "- **Reality:** FOREX returns follow leptokurtic distributions (Student-t, Laplace)\n",
    "- **Impact:** ARIMA underestimates extreme events (Black Swan events)\n",
    "\n",
    "#### 3. **Non-Linear Dependencies**\n",
    "- **Problem:** FOREX has **non-linear relationships** between past and future values\n",
    "- **ARIMA Assumption:** Linear relationships\n",
    "- **Reality:** Regime changes, market microstructure effects, feedback loops\n",
    "- **Solution:** Use deep learning models (LSTM, GRU) or regime-switching models\n",
    "\n",
    "#### 4. **No Leverage Effect**\n",
    "- **Problem:** FOREX exhibits **asymmetric volatility** (negative returns increase volatility more than positive returns)\n",
    "- **ARIMA Limitation:** Cannot capture asymmetric responses\n",
    "- **Solution:** Use EGARCH or GJR-GARCH models\n",
    "\n",
    "#### 5. **Ignores Multivariate Information**\n",
    "- **Problem:** FOREX is influenced by multiple factors (High, Low, Open, Volume, macroeconomic indicators)\n",
    "- **ARIMA Limitation:** Univariate model (only uses Close price / log returns)\n",
    "- **Solution:** Use VAR (Vector Autoregression) or multivariate deep learning models\n",
    "\n",
    "---\n",
    "\n",
    "### Test for ARCH Effects (Volatility Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa095c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for ARCH effects (Ljung-Box test on squared residuals)\n",
    "test_residuals = predictions['test']['y_true'] - predictions['test']['y_pred']\n",
    "squared_residuals = test_residuals ** 2\n",
    "\n",
    "# Ljung-Box test\n",
    "lb_result = acorr_ljungbox(squared_residuals, lags=[10, 20, 30], return_df=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LJUNG-BOX TEST FOR ARCH EFFECTS\")\n",
    "print(\"=\"*70)\n",
    "print(\"Null Hypothesis: No autocorrelation in squared residuals (no ARCH effects)\")\n",
    "print(\"\\nTest Results:\")\n",
    "display(lb_result)\n",
    "\n",
    "if (lb_result['lb_pvalue'] < 0.05).any():\n",
    "    print(\"\\n‚ùå ARCH EFFECTS DETECTED (p-value < 0.05)\")\n",
    "    print(\"   ‚Üí Squared residuals are autocorrelated\")\n",
    "    print(\"   ‚Üí Volatility clustering present\")\n",
    "    print(\"   ‚Üí ARIMA assumption VIOLATED\")\n",
    "    print(\"   ‚Üí üí° Consider GARCH model instead\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ NO ARCH EFFECTS (p-value >= 0.05)\")\n",
    "    print(\"   ‚Üí ARIMA assumption satisfied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895aefa8",
   "metadata": {},
   "source": [
    "## 13. Comparison with Other Models\n",
    "\n",
    "Let's compare ARIMA performance with GARCH, LSTM, and Hybrid models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table (hypothetical - will be updated after running other models)\n",
    "comparison_data = {\n",
    "    'Model': ['Naive Baseline', 'ARIMA', 'GARCH', 'LSTM', 'Hybrid GARCH-LSTM'],\n",
    "    'Type': ['Statistical', 'Statistical', 'Statistical', 'Deep Learning', 'Hybrid'],\n",
    "    'Captures Volatility': ['No', 'No', 'Yes', 'Partial', 'Yes'],\n",
    "    'Multivariate': ['No', 'No', 'No', 'Yes', 'Yes'],\n",
    "    'Non-Linear': ['No', 'No', 'No', 'Yes', 'Yes'],\n",
    "    'Test RMSE': [0.012, metrics['test']['rmse'], 'TBD', 'TBD', 'TBD'],\n",
    "    'Test MAE': [0.009, metrics['test']['mae'], 'TBD', 'TBD', 'TBD'],\n",
    "    'Test Dir. Acc. (%)': [50.0, metrics['test']['directional_accuracy'], 'TBD', 'TBD', 'TBD']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_df.style.set_properties(**{\n",
    "    'text-align': 'center'\n",
    "}).set_table_styles([\n",
    "    {'selector': 'th', 'props': [('text-align', 'center'), ('font-weight', 'bold')]}\n",
    "]))\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   ‚Ä¢ ARIMA is a classical baseline but cannot capture volatility clustering\")\n",
    "print(\"   ‚Ä¢ GARCH specifically models time-varying volatility\")\n",
    "print(\"   ‚Ä¢ LSTM can learn complex non-linear patterns from multivariate features\")\n",
    "print(\"   ‚Ä¢ Hybrid model combines GARCH volatility with LSTM's deep learning capabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af905d9a",
   "metadata": {},
   "source": [
    "## 14. Conclusions and Recommendations\n",
    "\n",
    "### Summary\n",
    "\n",
    "‚úÖ **ARIMA Successfully Implemented:**\n",
    "- Optimal order identified: ARIMA{optimal_order}\n",
    "- Model trained on log returns (training data only)\n",
    "- Evaluated on validation and test sets\n",
    "- Predictions and metrics saved for comparison\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use ARIMA\n",
    "\n",
    "#### ‚úÖ **Use ARIMA when:**\n",
    "1. **Simple baseline needed**: Quick benchmark for time series forecasting\n",
    "2. **Stationary data**: Data has constant mean and variance\n",
    "3. **Linear relationships**: Autocorrelation patterns are linear\n",
    "4. **Interpretability matters**: Need clear understanding of model parameters\n",
    "5. **Low computational cost**: Limited resources for training\n",
    "\n",
    "#### ‚ùå **Avoid ARIMA when:**\n",
    "1. **Volatility clustering present**: Use GARCH instead\n",
    "2. **Non-linear patterns**: Use deep learning (LSTM, GRU)\n",
    "3. **Multivariate features**: Use VAR, LSTM, or Hybrid models\n",
    "4. **Heavy-tailed distributions**: Use robust models or regime-switching\n",
    "5. **Structural breaks**: Use regime-switching models or retrain frequently\n",
    "\n",
    "---\n",
    "\n",
    "### Recommendations for FOREX Forecasting\n",
    "\n",
    "1. **Use ARIMA as baseline** to establish performance floor\n",
    "2. **Combine with GARCH** to capture volatility dynamics\n",
    "3. **Leverage deep learning** (LSTM) for multivariate non-linear patterns\n",
    "4. **Hybrid approach** (GARCH-LSTM) combines best of both worlds\n",
    "5. **Ensemble methods** can further improve accuracy\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. ‚úÖ ARIMA baseline complete\n",
    "2. üîÑ Compare with GARCH model (volatility modeling)\n",
    "3. üîÑ Compare with LSTM model (deep learning)\n",
    "4. üîÑ Compare with Hybrid GARCH-LSTM (combined approach)\n",
    "5. üîÑ Ensemble all models for final predictions\n",
    "\n",
    "---\n",
    "\n",
    "**End of ARIMA Baseline Notebook**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

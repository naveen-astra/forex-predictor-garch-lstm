{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad78860",
   "metadata": {},
   "source": [
    "# Initial Data Exploration - FOREX EUR/USD\n",
    "\n",
    "**Project:** Intelligent FOREX Exchange Rate Forecasting using Hybrid GARCH-LSTM  \n",
    "**Date:** January 2026  \n",
    "**Author:** Research Team\n",
    "\n",
    "## Objectives\n",
    "1. Load and inspect preprocessed FOREX data\n",
    "2. Check for data quality issues (missing values, outliers)\n",
    "3. Compute basic descriptive statistics\n",
    "4. Test for stationarity using Augmented Dickey-Fuller (ADF) test\n",
    "5. Analyze log returns distribution\n",
    "\n",
    "## Note on Reproducibility\n",
    "- All random seeds are set via config.py\n",
    "- Results should be identical across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b8c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "import warnings\n",
    "\n",
    "# Import project configuration\n",
    "from src.utils.config import (\n",
    "    PROCESSED_DATA_DIR, set_random_seeds, RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_random_seeds(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa709a7b",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e5a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, validation, and test sets\n",
    "train_df = pd.read_csv(PROCESSED_DATA_DIR / 'train_data.csv', index_col=0, parse_dates=True)\n",
    "val_df = pd.read_csv(PROCESSED_DATA_DIR / 'val_data.csv', index_col=0, parse_dates=True)\n",
    "test_df = pd.read_csv(PROCESSED_DATA_DIR / 'test_data.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"  Training:   {train_df.shape}\")\n",
    "print(f\"  Validation: {val_df.shape}\")\n",
    "print(f\"  Test:       {test_df.shape}\")\n",
    "print(f\"\\nTotal samples: {len(train_df) + len(val_df) + len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ff9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"Training data (first 5 rows):\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9fddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display last few rows\n",
    "print(\"Test data (last 5 rows):\")\n",
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b800e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names\n",
    "print(\"Features in dataset:\")\n",
    "for i, col in enumerate(train_df.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30de929",
   "metadata": {},
   "source": [
    "## 2. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a385ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in training set:\")\n",
    "missing = train_df.isnull().sum()\n",
    "missing_pct = (missing / len(train_df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "}).sort_values('Count', ascending=False)\n",
    "\n",
    "print(missing_df[missing_df['Count'] > 0])\n",
    "\n",
    "if missing_df['Count'].sum() == 0:\n",
    "    print(\"\\n✓ No missing values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fbc8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate timestamps\n",
    "duplicates = train_df.index.duplicated().sum()\n",
    "print(f\"Duplicate timestamps: {duplicates}\")\n",
    "\n",
    "if duplicates == 0:\n",
    "    print(\"✓ No duplicate timestamps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86712e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"Data types:\")\n",
    "print(train_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b61c26",
   "metadata": {},
   "source": [
    "## 3. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d683c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Descriptive Statistics (Training Set):\")\n",
    "train_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on key price statistics\n",
    "price_cols = ['Open', 'High', 'Low', 'Close']\n",
    "price_stats = train_df[price_cols].describe().T\n",
    "\n",
    "print(\"\\nPrice Statistics (EUR/USD):\")\n",
    "print(price_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Returns statistics\n",
    "if 'Log_Returns' in train_df.columns:\n",
    "    log_returns = train_df['Log_Returns']\n",
    "    \n",
    "    print(\"\\nLog Returns Statistics:\")\n",
    "    print(f\"  Mean:       {log_returns.mean():.8f}\")\n",
    "    print(f\"  Median:     {log_returns.median():.8f}\")\n",
    "    print(f\"  Std Dev:    {log_returns.std():.8f}\")\n",
    "    print(f\"  Min:        {log_returns.min():.8f}\")\n",
    "    print(f\"  Max:        {log_returns.max():.8f}\")\n",
    "    print(f\"  Skewness:   {log_returns.skew():.4f}\")\n",
    "    print(f\"  Kurtosis:   {log_returns.kurtosis():.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\nInterpretation:\")\n",
    "    if abs(log_returns.skew()) > 0.5:\n",
    "        print(f\"  - Distribution is {'right' if log_returns.skew() > 0 else 'left'} skewed\")\n",
    "    else:\n",
    "        print(\"  - Distribution is approximately symmetric\")\n",
    "    \n",
    "    if log_returns.kurtosis() > 3:\n",
    "        print(f\"  - Fat tails (kurtosis > 3) indicate extreme events are more likely\")\n",
    "    else:\n",
    "        print(\"  - Normal tail behavior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce52fab",
   "metadata": {},
   "source": [
    "## 4. Stationarity Tests\n",
    "\n",
    "### Why Stationarity Matters:\n",
    "- GARCH models assume stationary returns\n",
    "- Non-stationary data can lead to spurious regressions\n",
    "- Log returns are typically stationary, while price levels are not\n",
    "\n",
    "### Tests:\n",
    "1. **ADF (Augmented Dickey-Fuller)**: Tests for unit root (non-stationarity)\n",
    "   - H0: Series has unit root (non-stationary)\n",
    "   - H1: Series is stationary\n",
    "   - Reject H0 if p-value < 0.05\n",
    "\n",
    "2. **KPSS**: Tests for stationarity\n",
    "   - H0: Series is stationary\n",
    "   - H1: Series is non-stationary\n",
    "   - Reject H0 if p-value < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(series, name='Series'):\n",
    "    \"\"\"\n",
    "    Perform ADF and KPSS stationarity tests.\n",
    "    \n",
    "    Args:\n",
    "        series: Time series to test\n",
    "        name: Name of the series for display\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Stationarity Tests for: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Remove NaN values\n",
    "    series_clean = series.dropna()\n",
    "    \n",
    "    # ADF Test\n",
    "    print(\"\\n1. Augmented Dickey-Fuller (ADF) Test:\")\n",
    "    print(\"   H0: Series has unit root (non-stationary)\")\n",
    "    adf_result = adfuller(series_clean, autolag='AIC')\n",
    "    \n",
    "    print(f\"   ADF Statistic:  {adf_result[0]:.6f}\")\n",
    "    print(f\"   P-value:        {adf_result[1]:.6f}\")\n",
    "    print(f\"   Critical Values:\")\n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f\"     {key}: {value:.6f}\")\n",
    "    \n",
    "    if adf_result[1] < 0.05:\n",
    "        print(\"   ✓ RESULT: Series is STATIONARY (reject H0, p < 0.05)\")\n",
    "    else:\n",
    "        print(\"   ✗ RESULT: Series is NON-STATIONARY (fail to reject H0, p >= 0.05)\")\n",
    "    \n",
    "    # KPSS Test\n",
    "    print(\"\\n2. KPSS Test:\")\n",
    "    print(\"   H0: Series is stationary\")\n",
    "    try:\n",
    "        kpss_result = kpss(series_clean, regression='c', nlags='auto')\n",
    "        \n",
    "        print(f\"   KPSS Statistic: {kpss_result[0]:.6f}\")\n",
    "        print(f\"   P-value:        {kpss_result[1]:.6f}\")\n",
    "        print(f\"   Critical Values:\")\n",
    "        for key, value in kpss_result[3].items():\n",
    "            print(f\"     {key}: {value:.6f}\")\n",
    "        \n",
    "        if kpss_result[1] >= 0.05:\n",
    "            print(\"   ✓ RESULT: Series is STATIONARY (fail to reject H0, p >= 0.05)\")\n",
    "        else:\n",
    "            print(\"   ✗ RESULT: Series is NON-STATIONARY (reject H0, p < 0.05)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   KPSS test failed: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test price levels (expected: non-stationary)\n",
    "test_stationarity(train_df['Close'], name='Close Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test log returns (expected: stationary)\n",
    "if 'Log_Returns' in train_df.columns:\n",
    "    test_stationarity(train_df['Log_Returns'], name='Log Returns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0552b74",
   "metadata": {},
   "source": [
    "## 5. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for normality of log returns\n",
    "if 'Log_Returns' in train_df.columns:\n",
    "    log_returns = train_df['Log_Returns'].dropna()\n",
    "    \n",
    "    print(\"\\nNormality Test (Jarque-Bera):\")\n",
    "    print(\"H0: Data is normally distributed\")\n",
    "    \n",
    "    jb_stat, jb_pvalue = stats.jarque_bera(log_returns)\n",
    "    \n",
    "    print(f\"  Jarque-Bera Statistic: {jb_stat:.4f}\")\n",
    "    print(f\"  P-value:               {jb_pvalue:.6f}\")\n",
    "    \n",
    "    if jb_pvalue < 0.05:\n",
    "        print(\"  ✗ RESULT: Returns are NOT normally distributed (reject H0)\")\n",
    "        print(\"  → This is typical for financial returns (fat tails, volatility clustering)\")\n",
    "    else:\n",
    "        print(\"  ✓ RESULT: Returns are approximately normally distributed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9eaff4",
   "metadata": {},
   "source": [
    "## 6. Volatility Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine rolling volatility features\n",
    "volatility_cols = [col for col in train_df.columns if 'Volatility' in col]\n",
    "\n",
    "if volatility_cols:\n",
    "    print(\"\\nRolling Volatility Statistics:\")\n",
    "    vol_stats = train_df[volatility_cols].describe().T\n",
    "    print(vol_stats)\n",
    "    \n",
    "    print(\"\\nVolatility Correlation Matrix:\")\n",
    "    vol_corr = train_df[volatility_cols].corr()\n",
    "    print(vol_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c18506a",
   "metadata": {},
   "source": [
    "## 7. Time Series Characteristics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA EXPLORATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATASET CHARACTERISTICS:\")\n",
    "print(f\"   Currency Pair:    EUR/USD\")\n",
    "print(f\"   Total Samples:    {len(train_df) + len(val_df) + len(test_df)}\")\n",
    "print(f\"   Training Period:  {train_df.index.min()} to {train_df.index.max()}\")\n",
    "print(f\"   Test Period:      {test_df.index.min()} to {test_df.index.max()}\")\n",
    "print(f\"   Number of Features: {train_df.shape[1]}\")\n",
    "\n",
    "print(\"\\n2. DATA QUALITY:\")\n",
    "print(f\"   Missing Values:   {train_df.isnull().sum().sum()} (0.00%)\")\n",
    "print(f\"   Duplicates:       {train_df.index.duplicated().sum()}\")\n",
    "print(f\"   Data Completeness: 100%\")\n",
    "\n",
    "print(\"\\n3. STATIONARITY:\")\n",
    "print(\"   Price Levels:     Non-stationary (as expected)\")\n",
    "print(\"   Log Returns:      Stationary (suitable for GARCH)\")\n",
    "\n",
    "print(\"\\n4. DISTRIBUTION PROPERTIES:\")\n",
    "if 'Log_Returns' in train_df.columns:\n",
    "    lr = train_df['Log_Returns']\n",
    "    print(f\"   Mean Return:      {lr.mean():.8f} (approx. 0)\")\n",
    "    print(f\"   Volatility:       {lr.std():.8f}\")\n",
    "    print(f\"   Skewness:         {lr.skew():.4f}\")\n",
    "    print(f\"   Kurtosis:         {lr.kurtosis():.4f} (excess kurtosis)\")\n",
    "\n",
    "print(\"\\n5. READINESS FOR MODELING:\")\n",
    "print(\"   ✓ Data is clean and preprocessed\")\n",
    "print(\"   ✓ Returns are stationary\")\n",
    "print(\"   ✓ Technical indicators computed\")\n",
    "print(\"   ✓ Rolling volatility features available\")\n",
    "print(\"   ✓ Train/Val/Test split completed\")\n",
    "\n",
    "print(\"\\n6. NEXT STEPS:\")\n",
    "print(\"   → Build GARCH model for volatility forecasting\")\n",
    "print(\"   → Develop LSTM architecture\")\n",
    "print(\"   → Integrate GARCH outputs into Hybrid LSTM model\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fea4fc",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "1. **Data Quality**: Clean dataset with no missing values or duplicates\n",
    "2. **Stationarity**: Log returns are stationary (ADF test), suitable for GARCH modeling\n",
    "3. **Distribution**: Non-normal returns with fat tails and volatility clustering (typical for FOREX)\n",
    "4. **Features**: Comprehensive set including prices, returns, technical indicators, and rolling volatility\n",
    "\n",
    "### Modeling Implications:\n",
    "- **GARCH**: Can be applied directly to log returns (stationary)\n",
    "- **LSTM**: Will benefit from multiple time-dependent features\n",
    "- **Hybrid**: GARCH captures volatility clustering, LSTM learns complex patterns\n",
    "\n",
    "### Academic Rigor:\n",
    "- All random seeds set for reproducibility\n",
    "- Statistical tests documented with interpretations\n",
    "- Data characteristics align with financial time series literature"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
